{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    " \n",
    "base_path = './datasets/titanic/'\n",
    "\n",
    "train = pd.read_csv(base_path+'train.csv')\n",
    "test = pd.read_csv(base_path+'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-初步探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **PassengerId**: a unique identifier for each passenger\n",
    "* **Survived**: that's the target, 0 means the passenger did not survive, while 1 means he/she survived.\n",
    "* **Pclass**: passenger class.\n",
    "* **Name**, **Sex**, **Age**: self-explanatory\n",
    "* **SibSp**: how many siblings & spouses of the passenger aboard the Titanic.\n",
    "* **Parch**: how many children & parents of the passenger aboard the Titanic.\n",
    "* **Ticket**: ticket id\n",
    "* **Fare**: price paid (in pounds)\n",
    "* **Cabin**: passenger's cabin number\n",
    "* **Embarked**: where the passenger embarked the Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果可以看到，训练数据集有891个样本（样本量不大，要在模型训练过程中小心过拟合），11个特征和1个标签，其中特征‘Age'，'Cabin'，'Embarked'都有不同程度的缺损；测试集有418个样本，只有11个特征，其中特征'Age'，'Fare'，'Cabin'有不同程度的缺损。\n",
    "\n",
    "对这些缺损的数据可以选择的处理方式由简到难包括：\n",
    "1. 直接删除此特征（缺损数据太多的情况，防止引入噪声）\n",
    "2. 直接删除缺损数据的样本（~土豪操作~只用于训练数据集，且样本量较大，缺损数据样本较少的情况）\n",
    "3. 直接将有无数值作为新的特征（数据缺失较多，且数据有无本身是对预测是一个有用的特征）\n",
    "4. 中值或均值回补（缺失数据较多，不想损失此较多训练数据，特征又比较重要的情况，是比较常用的方法）\n",
    "5. 参考其他特征，利用与此特征的相关性编写算法回补数据（~大神级操作~回补的准确性可能会比较高一些，但实现过程复杂）\n",
    "\n",
    "这几种方法具体使用哪一个需要根据实际情况决定，选用复杂的方法得到的结果不一定就好。\n",
    "\n",
    "再来观察这11个特征的类型：\n",
    "* 其中有4个特征：'PassengerId'，'Pclass’,'Sibsp'，'Parch'属于**整数型数据**；\n",
    "* 5个特征：'Name'，'Sex'，'Ticket'，'Cabin'，'Embarked'属于**字符串类型数据**；\n",
    "* 2个特征：'Age'，'Fare'属于**浮点数**。\n",
    "\n",
    "然而这些数据格式并不都是机器学习模型的菜，不能直接喂给模型字符串数据。\n",
    "为统一数据格式，方便模型训练，我们下面还需要对这些特征数据进行缩放和转化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-特征分析与处理\n",
    "有些可能要用python的**map函数进行特征分层**，非常实用！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 乘客阶级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#采用seaborn绘图函数库作可视化分析\n",
    "sns.countplot(x=\"Pclass\", hue=\"Survived\", data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 性别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Sex\", hue=\"Survived\", data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 年龄（缺失值用no表示）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Age'].isnull().sum() #统计缺失值个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将有年龄数值的转化为yes,缺损的转化为no\n",
    "train['Age']=train['Age'].map(lambda x:'yes' if 0<x<100 else 'no')\n",
    "sns.countplot(x=\"Age\", hue=\"Survived\", data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('./datasets/titanic/train.csv')\n",
    "sns.violinplot(x='Survived',y='Age',data=train)  #小提琴图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Age']=train['Age'].map(lambda x: 'child' if x<12 \n",
    "            else 'youth' if x<30 \n",
    "            else 'adlut' if x<60 \n",
    "            else 'old' if x<75 \n",
    "            else 'tooold' if x>=75 \n",
    "            else 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 兄弟姐妹数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"SibSp\", hue=\"Survived\", data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SibSp']=train['SibSp'].map(lambda x: 'small' if x<1 \n",
    "                else 'middle' if x<3 \n",
    "                else 'large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 父母孩子数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Parch\", hue=\"Survived\", data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Parch']=train['Parch'].map(lambda x: 'small' if x<1 \n",
    "                else 'middle' if x<4 \n",
    "                else 'large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 船票价格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='Survived',y='Fare',data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#因为原图效果不明显，做对数变换\n",
    "train['Fare']=train['Fare'].map(lambda x:np.log(x+1))  #用numpy库里的对数函数对Fare的数值进行对数转换\n",
    "sns.violinplot(x='Survived',y='Fare',data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 船票价格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Fare']=train['Fare'].map(lambda x: 'poor' if x<2.5 else 'rich')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 船舱编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cabin']=train['Cabin'].map(lambda x:'yes' if type(x)==str else 'no')\n",
    "sns.countplot(x=\"Cabin\", hue=\"Survived\", data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上船港口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Embarked\", hue=\"Survived\", data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Embarked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(axis=0,inplace=True) #删掉含有缺损值的样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据大致浏览完成，查看现在训练集的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此特征分析全部完成，可以删除的特征：'PassengerId'，'Name'和'Ticket'。(更专业的人可以选择对这几个特征再做处理)\n",
    "\n",
    "然后对剩余的特征进行独热编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将训练数据分成标记和特征两部分\n",
    "labels= train['Survived']\n",
    "features= train.drop(['Survived','PassengerId','Name','Ticket'],axis=1)\n",
    "\n",
    "#对所有特征实现独热编码\n",
    "features = pd.get_dummies(features)\n",
    "encoded = list(features.columns)\n",
    "print (\"{} total features after one-hot encoding.\".format(len(encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 对测试集进行同样的处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对'Age','SibSp'，'Parch'特征分段分类\n",
    "test['Age']=test['Age'].map(lambda x: 'child' if x<12 else 'youth' if x<30 else 'adlut' if x<60 else 'old' if x<75 else 'tooold' if x>=75 else 'null')\n",
    "test['SibSp']=test['SibSp'].map(lambda x: 'small' if x<1 else 'middle' if x<3 else 'large')\n",
    "test['Parch']=test['Parch'].map(lambda x: 'small' if x<1 else 'middle' if x<4 else 'large')\n",
    "#均值补齐'Fare'特征值对数转换和分类\n",
    "test.Fare.fillna(test['Fare'].mean(), inplace=True)\n",
    "test['Fare']=test['Fare'].map(lambda x:np.log(x+1))\n",
    "test['Fare']=test['Fare'].map(lambda x: 'poor' if x<2.5 else 'rich')\n",
    "#按'Cabin'是否缺损分类\n",
    "test['Cabin']=test['Cabin'].map(lambda x:'yes' if type(x)==str else 'no')\n",
    "#删除不需要的特征并进行独热编码\n",
    "Id=test['PassengerId']\n",
    "test=test.drop(['PassengerId','Name','Ticket'],axis=1)\n",
    "test=pd.get_dummies(test)\n",
    "encoded = list(test.columns)\n",
    "print (\"{} total features after one-hot encoding.\".format(len(encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑要用到的算法包括：决策树，SVM，随机森林，Adaboost，KNN以及传说中的大杀器Xgboost。\n",
    "\n",
    "**先建立一个统一的训练框架**，方便之后**网格搜索**调参。\n",
    "\n",
    "**这个思路很好，可以借鉴学习！！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from time import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# 通用函数框架\n",
    "def fit_model(alg,parameters):\n",
    "    X=features\n",
    "    y=labels  #由于数据较少，使用全部数据进行网格搜索\n",
    "    scorer=make_scorer(roc_auc_score)  #使用roc_auc_score作为评分标准\n",
    "    grid = GridSearchCV(alg,parameters,scoring=scorer,cv=5)  #使用网格搜索，出入参数\n",
    "    start=time()  #计时\n",
    "    grid=grid.fit(X,y)  #模型训练\n",
    "    end=time()\n",
    "    t=round(end-start,3)\n",
    "    print (grid.best_params_)  #输出最佳参数\n",
    "    print ('searching time for {} is {} s'.format(alg.__class__.__name__,t)) #输出搜索时间\n",
    "    return grid #返回训练好的模型\n",
    "\n",
    "#列出需要使用的算法\n",
    "alg1=DecisionTreeClassifier(random_state=29)\n",
    "alg2=SVC(probability=True,random_state=29)  #由于使用roc_auc_score作为评分标准，需将SVC中的probability参数设置为True\n",
    "alg3=RandomForestClassifier(random_state=29)\n",
    "alg4=AdaBoostClassifier(random_state=29)\n",
    "alg5=KNeighborsClassifier(n_jobs=-1)\n",
    "alg6=XGBClassifier(random_state=29,n_jobs=-1,use_label_encoder=False)\n",
    "\n",
    "# 列出需要调整的参数范围\n",
    "#第一版-与第二版不同处备注：\n",
    "# parameters1={'max_depth':range(1,10),'min_samples_split':range(2,10)}\n",
    "# parameters2 = {\"C\":range(1,20), \"gamma\": [0.05,0.1,0.15,0.2,0.25]}\n",
    "# parameters3_2 = {'max_depth':range(1,10),'min_samples_split':range(2,10)}  #搜索空间太大，分两次调整参数\n",
    "# parameters5 = {'n_neighbors':range(2,10),'leaf_size':range(10,80,20) }\n",
    "\n",
    "#第二版-\n",
    "parameters1={'max_depth':range(1,10),'min_samples_split':range(1,10)}\n",
    "parameters2 = {\"C\":range(1,20), \"gamma\": [0.01,0.02,0.05,0.1,0.15]}\n",
    "parameters3_1 = {'n_estimators':range(10,200,10)}\n",
    "parameters3_2 = {'max_depth':range(1,10),'min_samples_split':range(1,8)}  #搜索空间太大，分两次调整参数\n",
    "parameters4 = {'n_estimators':range(10,200,10),'learning_rate':[i/10.0 for i in range(5,15)]}\n",
    "parameters5 = {'n_neighbors':range(2,10),'leaf_size':range(5,60,20)  }\n",
    "parameters6_1 = {'n_estimators':range(10,200,10)}\n",
    "parameters6_2 = {'max_depth':range(1,10),'min_child_weight':range(1,10)}\n",
    "parameters6_3 = {'subsample':[i/10.0 for i in range(1,10)], \n",
    "                'colsample_bytree':[i/10.0 for i in range(1,10)]}#搜索空间太大，分三次调整参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**调参范围（可能并非最优，还可以改进）**\n",
    "\n",
    "列出我们需要调整的参数及取值范围，这是一个很繁琐的工作，需要大量的尝试和优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来**开始调参**（炼丹）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1=fit_model(alg1,parameters1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2=fit_model(alg2,parameters2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3_m1=fit_model(alg3,parameters3_1) #第一次调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg3=RandomForestClassifier(random_state=29,n_estimators=180) #第二次调参\n",
    "clf3=fit_model(alg3,parameters3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4=fit_model(alg4,parameters4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5=fit_model(alg5,parameters5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf6_m1=fit_model(alg6,parameters6_1) #第一次调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg6=XGBClassifier(n_estimators=140,random_state=29,n_jobs=-1) #第二次调参\n",
    "clf6_m2=fit_model(alg6,parameters6_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg6=XGBClassifier(n_estimators=140,max_depth=4,min_child_weight=5,random_state=29,n_jobs=-1) #第三次调参\n",
    "clf6=fit_model(alg6,parameters6_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-验证结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个保存函数，将预测的结果保存为可以提交的格式；然后调用这个函数，完成6个模型的预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pth = './datasets/titanic/'\n",
    "\n",
    "def save(clf,i):\n",
    "    pred=clf.predict(test)\n",
    "    sub=pd.DataFrame({ 'PassengerId': Id, 'Survived': pred })\n",
    "    sub.to_csv(base_pth + \"res_tan_{}.csv\".format(i), index=False)\n",
    "    \n",
    "i=1\n",
    "for clf in [clf1,clf2,clf3,clf4,clf5,clf6]:\n",
    "    save(clf,i)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后将个模型预测结果csv文件提交至kaggle。\n",
    "\n",
    "-------------------------\n",
    "\n",
    "* 本文参考作者链接：https://blog.csdn.net/aicanghai_smile/article/details/79234172\n",
    "* 特此感谢！\n",
    "  \n",
    "参考博主的预测成绩**（和我有点不同）**：6个预测结果都超过了0.77，基本达到预测的效果，成绩最好的是随机森林模型，得分0.79425，比较出乎意外的是Xgboost的算法成绩竟然只有0.77511，可能是参数没有调好。大家可以尝试其他参数，说不定可以得到更好的成绩。不过考虑到这个项目数据量太小，能到0.8左右的成绩应该已经比较好了，重要的是学习数据处理，特征分析以及模型构建调参的过程，目的已经达到。\n",
    "\n",
    "12.23更新：我的第二版预测结果见 ./dataset/titanic/result.png\n",
    "------------------------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86007600e5b6c9a1da171e7306281d2c726c74e7e11364567bf42e8cc5d33bac"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
